"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8115],{3905:function(a,t,e){e.d(t,{Zo:function(){return u},kt:function(){return k}});var l=e(7294);function n(a,t,e){return t in a?Object.defineProperty(a,t,{value:e,enumerable:!0,configurable:!0,writable:!0}):a[t]=e,a}function s(a,t){var e=Object.keys(a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(a);t&&(l=l.filter((function(t){return Object.getOwnPropertyDescriptor(a,t).enumerable}))),e.push.apply(e,l)}return e}function r(a){for(var t=1;t<arguments.length;t++){var e=null!=arguments[t]?arguments[t]:{};t%2?s(Object(e),!0).forEach((function(t){n(a,t,e[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(e)):s(Object(e)).forEach((function(t){Object.defineProperty(a,t,Object.getOwnPropertyDescriptor(e,t))}))}return a}function d(a,t){if(null==a)return{};var e,l,n=function(a,t){if(null==a)return{};var e,l,n={},s=Object.keys(a);for(l=0;l<s.length;l++)e=s[l],t.indexOf(e)>=0||(n[e]=a[e]);return n}(a,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(a);for(l=0;l<s.length;l++)e=s[l],t.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(a,e)&&(n[e]=a[e])}return n}var i=l.createContext({}),o=function(a){var t=l.useContext(i),e=t;return a&&(e="function"==typeof a?a(t):r(r({},t),a)),e},u=function(a){var t=o(a.components);return l.createElement(i.Provider,{value:t},a.children)},c={inlineCode:"code",wrapper:function(a){var t=a.children;return l.createElement(l.Fragment,{},t)}},p=l.forwardRef((function(a,t){var e=a.components,n=a.mdxType,s=a.originalType,i=a.parentName,u=d(a,["components","mdxType","originalType","parentName"]),p=o(e),k=n,m=p["".concat(i,".").concat(k)]||p[k]||c[k]||s;return e?l.createElement(m,r(r({ref:t},u),{},{components:e})):l.createElement(m,r({ref:t},u))}));function k(a,t){var e=arguments,n=t&&t.mdxType;if("string"==typeof a||n){var s=e.length,r=new Array(s);r[0]=p;var d={};for(var i in t)hasOwnProperty.call(t,i)&&(d[i]=t[i]);d.originalType=a,d.mdxType="string"==typeof a?a:n,r[1]=d;for(var o=2;o<s;o++)r[o]=e[o];return l.createElement.apply(null,r)}return l.createElement.apply(null,e)}p.displayName="MDXCreateElement"},4861:function(a,t,e){e.r(t),e.d(t,{frontMatter:function(){return d},contentTitle:function(){return i},metadata:function(){return o},assets:function(){return u},toc:function(){return c},default:function(){return k}});var l=e(7462),n=e(3366),s=(e(7294),e(3905)),r=["components"],d={title:"classy.data.data_modules",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},i=void 0,o={unversionedId:"api/data/data_modules",id:"api/data/data_modules",title:"classy.data.data_modules",description:"Functions",source:"@site/docs/api/data/data_modules.md",sourceDirName:"api/data",slug:"/api/data/data_modules",permalink:"/classy/docs/api/data/data_modules",tags:[],version:"current",frontMatter:{title:"classy.data.data_modules",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},sidebar:"apiSidebar"},u={},c=[{value:"Functions",id:"functions",level:2},{value:"path_if_exists",id:"path_if_exists",level:3},{value:"Classes",id:"clzs",level:2},{value:"ClassyDataModule",id:"ClassyDataModule",level:3},{value:"__init__",id:"ClassyDataModule-init",level:4},{value:"get_examples",id:"ClassyDataModule-get_examples",level:4},{value:"prepare_data",id:"ClassyDataModule-prepare_data",level:4},{value:"setup",id:"ClassyDataModule-setup",level:4},{value:"test_dataloader",id:"ClassyDataModule-test_dataloader",level:4},{value:"train_dataloader",id:"ClassyDataModule-train_dataloader",level:4},{value:"val_dataloader",id:"ClassyDataModule-val_dataloader",level:4}],p={toc:c};function k(a){var t=a.components,e=(0,n.Z)(a,r);return(0,s.kt)("wrapper",(0,l.Z)({},p,e,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h2",{id:"functions"},"Functions"),(0,s.kt)("div",{className:"api"},(0,s.kt)("h3",{id:"path_if_exists"},"path_if_exists"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"path_if_exists"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0path:\xa0str,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0data_driver:\xa0",(0,s.kt)("a",{title:"DataDriver",href:"/docs/api/data/data_drivers#DataDriver"},"DataDriver"),",",(0,s.kt)("br",null),") \u2011>\xa0Optional[str]",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#path_if_exists",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L19-L20",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"}))),(0,s.kt)("h2",{id:"clzs"},"Classes"),(0,s.kt)("div",{className:"api"},(0,s.kt)("h3",{id:"ClassyDataModule"},"ClassyDataModule"),(0,s.kt)("div",{className:"api__signature"},(0,s.kt)("p",null,"class ",(0,s.kt)("span",{className:"ident"},"ClassyDataModule"),"(pytorch_lightning.core.datamodule.LightningDataModule)"),(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L23-L291",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models."),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"class MyDataModule(LightningDataModule): def __init__(self): super().__init__() def prepare_data(self): # download, split, etc... # only called on 1 GPU/TPU in distributed def setup(self, stage): # make assignments here (val/train/test split) # called on every process in DDP def train_dataloader(self): train_split = Dataset(...) return DataLoader(train_split) def val_dataloader(self): val_split = Dataset(...) return DataLoader(val_split) def test_dataloader(self): test_split = Dataset(...) return DataLoader(test_split) def teardown(self): # clean up after fit or test # called on every process in DDP")),(0,s.kt)("p",null,"A DataModule implements 6 key methods:"),(0,s.kt)("ul",null,(0,s.kt)("li",null,"prepare_data (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode)."),(0,s.kt)("li",null,"setup  (things to do on every accelerator in distributed mode)."),(0,s.kt)("li",null,"train_dataloader the training dataloader."),(0,s.kt)("li",null,"val_dataloader the val dataloader(s)."),(0,s.kt)("li",null,"test_dataloader the test dataloader(s)."),(0,s.kt)("li",null,"teardown (things to do on every accelerator in distributed mode when finished)")),(0,s.kt)("p",null,"This allows you to share a full dataset without explaining how to download, split, transform, and process the data")),(0,s.kt)("h4",{id:"ClassyDataModule-init"},"_","_","init","_","_"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"__init__"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0task:\xa0str,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0dataset_path:\xa0str,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0train_dataset:\xa0omegaconf.dictconfig.DictConfig,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0validation_dataset:\xa0Optional[omegaconf.dictconfig.DictConfig]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0test_dataset:\xa0Optional[omegaconf.dictconfig.DictConfig]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0validation_split_size:\xa0Optional[float]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0test_split_size:\xa0Optional[float]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0max_nontrain_split_size:\xa0Optional[int]\xa0=\xa0None,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0shuffle_dataset:\xa0bool\xa0=\xa0True,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0external_vocabulary_path:\xa0Optional[str]\xa0=\xa0None,",(0,s.kt)("br",null),")",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-init",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L23-L291",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__description"}),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-get_examples"},"get_examples"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"get_examples"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0n:\xa0int,",(0,s.kt)("br",null),") \u2011>\xa0Tuple[str,\xa0List[~T]]",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-get_examples",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L84-L89",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"}))),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-prepare_data"},"prepare_data"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"prepare_data"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),") \u2011>\xa0None",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-prepare_data",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L91-L253",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"Use this to download and prepare data."),(0,s.kt)("div",{class:"admonition warning"},(0,s.kt)("p",{class:"admonition-title"},"Warning:\u2002DO NOT set state to the model (use ",(0,s.kt)("code",null,"setup")," instead)"),(0,s.kt)("p",null,"since this is NOT called on every GPU in DDP/TPU")),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"def prepare_data(self): # good download_data() tokenize() etc()",(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"# bad\nself.split = data_split\nself.some_state = some_other_state()\n")))),(0,s.kt)("p",null,"In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):"),(0,s.kt)("ol",null,(0,s.kt)("li",null,"Once per node. This is the default and is only called on LOCAL_RANK=0."),(0,s.kt)("li",null,"Once in total. Only called on GLOBAL_RANK=0.")),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"# DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True)",(0,s.kt)("h1",{id:"call-on-global_rank0-great-for-shared-file-systems"},"call on GLOBAL_RANK=0 (great for shared file systems)"),(0,s.kt)("p",null,"Trainer(prepare_data_per_node=False)"))),(0,s.kt)("h5",{id:"note"},"Note"),(0,s.kt)("p",null,"Setting ",(0,s.kt)("code",null,"prepare_data_per_node")," with the trainer flag is deprecated and will be removed in v1.7.0. Please set ",(0,s.kt)("code",null,"prepare_data_per_node")," in LightningDataModule or LightningModule directly instead."),(0,s.kt)("p",null,"This is called before requesting the dataloaders:"),(0,s.kt)("p",null,".. code-block:: python"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()"))))),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-setup"},"setup"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"setup"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0stage:\xa0Optional[str]\xa0=\xa0None,",(0,s.kt)("br",null),") \u2011>\xa0None",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-setup",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L255-L276",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP."),(0,s.kt)("h5",{id:"args"},"Args"),(0,s.kt)("dl",null,(0,s.kt)("dt",null,(0,s.kt)("code",null,"stage")),(0,s.kt)("dd",null,"either ",(0,s.kt)("code",null,"'fit'"),", ",(0,s.kt)("code",null,"'validate'"),", ",(0,s.kt)("code",null,"'test'"),", or ",(0,s.kt)("code",null,"'predict'"))),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"class LitModel(...): def __init__(self): self.l1 = None",(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"def prepare_data(self):\n    download_data()\n    tokenize()\n\n    # don't do this\n    self.something = else\n\ndef setup(stage):\n    data = Load_data(...)\n    self.l1 = nn.Linear(28, data.num_classes)\n"))))))),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-test_dataloader"},"test_dataloader"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"test_dataloader"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),") \u2011>\xa0Union[torch.utils.data.dataloader.DataLoader,\xa0List[torch.utils.data.dataloader.DataLoader],\xa0Dict[str,\xa0torch.utils.data.dataloader.DataLoader]]",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-test_dataloader",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L288-L291",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"Implement one or multiple PyTorch DataLoaders for testing."),(0,s.kt)("p",null,"The dataloader you return will not be reloaded unless you set :paramref:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs")," to a postive integer."),(0,s.kt)("p",null,"For data processing use the following pattern:"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"- download in :meth:<code>prepare\\_data</code> - process and split in :meth:<code>setup</code>")),(0,s.kt)("p",null,"However, the above are only necessary for distributed processing."),(0,s.kt)("div",{class:"admonition warning"},(0,s.kt)("p",{class:"admonition-title"},"Warning:\u2002do not assign state in prepare_data")),(0,s.kt)("ul",null,(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.fit")),(0,s.kt)("li",null,"\u2026"),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"prepare_data")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"setup")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"train_dataloader")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"val_dataloader")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"test_dataloader"))),(0,s.kt)("h5",{id:"note"},"Note"),(0,s.kt)("p",null,"Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself."),(0,s.kt)("h5",{id:"return"},"Return"),(0,s.kt)("p",null,"A :class:",(0,s.kt)("code",null,"torch.utils.data.DataLoader")," or a sequence of them specifying testing samples."),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False )",(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"return loader\n")),(0,s.kt)("h1",{id:"can-also-return-multiple-dataloaders"},"can also return multiple dataloaders"),(0,s.kt)("p",null,"def test_dataloader(self):\nreturn ","[loader_a, loader_b, ..., loader_n]"))),(0,s.kt)("h5",{id:"note_1"},"Note"),(0,s.kt)("p",null,"If you don't need a test dataset and a :meth:",(0,s.kt)("code",null,"test_step"),", you don't need to implement this method."),(0,s.kt)("h5",{id:"note_2"},"Note"),(0,s.kt)("p",null,"In the case where you return multiple test dataloaders, the :meth:",(0,s.kt)("code",null,"test_step"),"will have an argument ",(0,s.kt)("code",null,"dataloader_idx")," which matches the order here.")))),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-train_dataloader"},"train_dataloader"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"train_dataloader"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),") \u2011>\xa0Union[torch.utils.data.dataloader.DataLoader,\xa0List[torch.utils.data.dataloader.DataLoader],\xa0Dict[str,\xa0torch.utils.data.dataloader.DataLoader]]",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-train_dataloader",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L278-L281",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"Implement one or more PyTorch DataLoaders for training."),(0,s.kt)("h5",{id:"return"},"Return"),(0,s.kt)("p",null,"A collection of :class:",(0,s.kt)("code",null,"torch.utils.data.DataLoader")," specifying training samples. In the case of multiple dataloaders, please see this :ref:",(0,s.kt)("code",null,"page <multiple-training-dataloaders>"),"."),(0,s.kt)("p",null,"The dataloader you return will not be reloaded unless you set :paramref:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs")," to a positive integer."),(0,s.kt)("p",null,"For data processing use the following pattern:"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"- download in :meth:<code>prepare\\_data</code> - process and split in :meth:<code>setup</code>")),(0,s.kt)("p",null,"However, the above are only necessary for distributed processing."),(0,s.kt)("div",{class:"admonition warning"},(0,s.kt)("p",{class:"admonition-title"},"Warning:\u2002do not assign state in prepare_data")),(0,s.kt)("ul",null,(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.fit")),(0,s.kt)("li",null,"\u2026"),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"prepare_data")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"setup")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"train_dataloader"))),(0,s.kt)("h5",{id:"note"},"Note"),(0,s.kt)("p",null,"Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself."),(0,s.kt)("p",null,"Example::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"# single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader",(0,s.kt)("h1",{id:"multiple-dataloaders-return-as-list"},"multiple dataloaders, return as list"),(0,s.kt)("p",null,"def train_dataloader(self):\nmnist = MNIST(...)\ncifar = CIFAR(...)\nmnist_loader = torch.utils.data.DataLoader(\ndataset=mnist, batch_size=self.batch_size, shuffle=True\n)\ncifar_loader = torch.utils.data.DataLoader(\ndataset=cifar, batch_size=self.batch_size, shuffle=True\n)"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"# each batch will be a list of tensors: [batch_mnist, batch_cifar]\nreturn [mnist_loader, cifar_loader]\n")),(0,s.kt)("h1",{id:"multiple-dataloader-return-as-dict"},"multiple dataloader, return as dict"),(0,s.kt)("p",null,"def train_dataloader(self):\nmnist = MNIST(...)\ncifar = CIFAR(...)\nmnist_loader = torch.utils.data.DataLoader(\ndataset=mnist, batch_size=self.batch_size, shuffle=True\n)\ncifar_loader = torch.utils.data.DataLoader(\ndataset=cifar, batch_size=self.batch_size, shuffle=True\n)"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"# each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar}\nreturn {'mnist': mnist_loader, 'cifar': cifar_loader}\n"))))))),(0,s.kt)("div",{className:"api"},(0,s.kt)("h4",{id:"ClassyDataModule-val_dataloader"},"val_dataloader"),(0,s.kt)("div",{className:"api__signature"},"def ",(0,s.kt)("span",{className:"ident"},"val_dataloader"),"(",(0,s.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,s.kt)("br",null),") \u2011>\xa0Union[torch.utils.data.dataloader.DataLoader,\xa0List[torch.utils.data.dataloader.DataLoader],\xa0Dict[str,\xa0torch.utils.data.dataloader.DataLoader]]",(0,s.kt)("div",{className:"links-div"},(0,s.kt)("a",{href:"#ClassyDataModule-val_dataloader",className:"direct-link"},"#"),(0,s.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/data_modules.py#L283-L286",className:"git-link"},"#"))),(0,s.kt)("div",{className:"api__body"},(0,s.kt)("div",{className:"api__description"},(0,s.kt)("p",null,"Implement one or multiple PyTorch DataLoaders for validation."),(0,s.kt)("p",null,"The dataloader you return will not be reloaded unless you set :paramref:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs")," to a positive integer."),(0,s.kt)("p",null,"It's recommended that all data downloads and preparation happen in :meth:",(0,s.kt)("code",null,"prepare_data"),"."),(0,s.kt)("ul",null,(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"~pytorch_lightning.trainer.Trainer.fit")),(0,s.kt)("li",null,"\u2026"),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"prepare_data")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"train_dataloader")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"val_dataloader")),(0,s.kt)("li",null,":meth:",(0,s.kt)("code",null,"test_dataloader"))),(0,s.kt)("h5",{id:"note"},"Note"),(0,s.kt)("p",null,"Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself."),(0,s.kt)("h5",{id:"return"},"Return"),(0,s.kt)("p",null,"A :class:",(0,s.kt)("code",null,"torch.utils.data.DataLoader")," or a sequence of them specifying validation samples."),(0,s.kt)("p",null,"Examples::"),(0,s.kt)("pre",null,(0,s.kt)("code",null,"def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False )",(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"return loader\n")),(0,s.kt)("h1",{id:"can-also-return-multiple-dataloaders-1"},"can also return multiple dataloaders"),(0,s.kt)("p",null,"def val_dataloader(self):\nreturn ","[loader_a, loader_b, ..., loader_n]"))),(0,s.kt)("h5",{id:"note_1"},"Note"),(0,s.kt)("p",null,"If you don't need a validation dataset and a :meth:",(0,s.kt)("code",null,"validation_step"),", you don't need to implement this method."),(0,s.kt)("h5",{id:"note_2"},"Note"),(0,s.kt)("p",null,"In the case where you return multiple validation dataloaders, the :meth:",(0,s.kt)("code",null,"validation_step"),"will have an argument ",(0,s.kt)("code",null,"dataloader_idx")," which matches the order here.")))))))}k.isMDXComponent=!0}}]);