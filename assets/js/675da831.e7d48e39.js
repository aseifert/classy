"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9942],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return p}});var r=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,s=function(e,t){if(null==e)return{};var a,r,s={},n=Object.keys(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var o=r.createContext({}),d=function(e){var t=r.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=d(e.components);return r.createElement(o.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var a=e.components,s=e.mdxType,n=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),f=d(a),p=s,_=f["".concat(o,".").concat(p)]||f[p]||u[p]||n;return a?r.createElement(_,i(i({ref:t},c),{},{components:a})):r.createElement(_,i({ref:t},c))}));function p(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var n=a.length,i=new Array(n);i[0]=f;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l.mdxType="string"==typeof e?e:s,i[1]=l;for(var d=2;d<n;d++)i[d]=a[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}f.displayName="MDXCreateElement"},38:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return l},contentTitle:function(){return o},metadata:function(){return d},assets:function(){return c},toc:function(){return u},default:function(){return p}});var r=a(7462),s=a(3366),n=(a(7294),a(3905)),i=["components"],l={title:"classy.data.dataset.hf.base",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},o=void 0,d={unversionedId:"api/data/dataset/hf/base",id:"api/data/dataset/hf/base",title:"classy.data.dataset.hf.base",description:"Classes",source:"@site/docs/api/data/dataset/hf/base.md",sourceDirName:"api/data/dataset/hf",slug:"/api/data/dataset/hf/base",permalink:"/classy/docs/api/data/dataset/hf/base",tags:[],version:"current",frontMatter:{title:"classy.data.dataset.hf.base",toc_min_heading_level:2,toc_max_heading_level:4,pagination_next:null,pagination_prev:null},sidebar:"apiSidebar"},c={},u=[{value:"Classes",id:"clzs",level:2},{value:"HFBaseDataset",id:"HFBaseDataset",level:3},{value:"__init__",id:"HFBaseDataset-init",level:4},{value:"init_fields_batcher",id:"HFBaseDataset-init_fields_batcher",level:4}],f={toc:u};function p(e){var t=e.components,a=(0,s.Z)(e,i);return(0,n.kt)("wrapper",(0,r.Z)({},f,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h2",{id:"clzs"},"Classes"),(0,n.kt)("div",{className:"api"},(0,n.kt)("h3",{id:"HFBaseDataset"},"HFBaseDataset"),(0,n.kt)("div",{className:"api__signature"},(0,n.kt)("p",null,"class ",(0,n.kt)("span",{className:"ident"},"HFBaseDataset"),"()"),(0,n.kt)("div",{className:"links-div"},(0,n.kt)("a",{href:"#HFBaseDataset",className:"direct-link"},"#"),(0,n.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/dataset/hf/base.py#L11-L42",className:"git-link"},"#"))),(0,n.kt)("div",{className:"api__body"},(0,n.kt)("div",{className:"api__description"},(0,n.kt)("p",null,"An iterable Dataset."),(0,n.kt)("p",null,"All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream."),(0,n.kt)("p",null,"All subclasses should overwrite :meth:",(0,n.kt)("code",null,"__iter__"),", which would return an iterator of samples in this dataset."),(0,n.kt)("p",null,"When a subclass is used with :class:",(0,n.kt)("code",null,"~torch.utils.data.DataLoader"),", each item in the dataset will be yielded from the :class:",(0,n.kt)("code",null,"~torch.utils.data.DataLoader"),"iterator. When :attr:",(0,n.kt)("code",null,"num_workers > 0"),", each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:",(0,n.kt)("code",null,"~torch.utils.data.get_worker_info"),", when called in a worker process, returns information about the worker. It can be used in either the dataset's :meth:",(0,n.kt)("code",null,"__iter__")," method or the :class:",(0,n.kt)("code",null,"~torch.utils.data.DataLoader")," 's :attr:",(0,n.kt)("code",null,"worker_init_fn")," option to modify each copy's behavior."),(0,n.kt)("p",null,"Example 1: splitting workload across all workers in :meth:",(0,n.kt)("code",null,"__iter__"),"::"),(0,n.kt)("pre",null,(0,n.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,n.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]"),(0,n.kt)("p",null,">",">",">"," # Mult-process loading with two worker processes\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 5, 4, 6]"),(0,n.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n","[3, 4, 5, 6]"))),(0,n.kt)("p",null,"Example 2: splitting workload across all workers using :attr:",(0,n.kt)("code",null,"worker_init_fn"),"::"),(0,n.kt)("pre",null,(0,n.kt)("code",null,'>>> class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end > start, "this example code only works with end >= start" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. >>> ds = MyIterableDataset(start=3, end=7)',(0,n.kt)("p",null,">",">",">"," # Single-process loading\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n","[3, 4, 5, 6]","\n",">",">",">","\n",">",">",">"," # Directly doing multi-process loading yields duplicate data\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n","[3, 3, 4, 4, 5, 5, 6, 6]"),(0,n.kt)("p",null,">",">",">"," # Define a ","<","code",">","worker","_","init","_","fn","<","/code",">"," that configures each dataset copy differently\n",">",">",">"," def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n..."),(0,n.kt)("p",null,">",">",">"," # Mult-process loading with the custom ","<","code",">","worker","_","init","_","fn","<","/code",">","\n",">",">",">"," # Worker 0 fetched ","[3, 4]",".  Worker 1 fetched ","[5, 6]",".\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n","[3, 5, 4, 6]"),(0,n.kt)("p",null,">",">",">"," # With even more workers\n",">",">",">"," print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n","[3, 4, 5, 6]")))),(0,n.kt)("details",null,(0,n.kt)("summary",null,"Subclasses (3)"),(0,n.kt)("div",null,(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{title:"HFQADataset",href:"/docs/api/data/dataset/hf/classification#HFQADataset"},"HFQADataset")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{title:"HFSequenceDataset",href:"/docs/api/data/dataset/hf/classification#HFSequenceDataset"},"HFSequenceDataset")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{title:"HFTokenDataset",href:"/docs/api/data/dataset/hf/classification#HFTokenDataset"},"HFTokenDataset"))))),(0,n.kt)("h4",{id:"HFBaseDataset-init"},"_","_","init","_","_"),(0,n.kt)("div",{className:"api__signature"},"def ",(0,n.kt)("span",{className:"ident"},"__init__"),"(",(0,n.kt)("br",null),"\xa0\xa0\xa0\xa0transformer_model:\xa0str,",(0,n.kt)("br",null),"\xa0\xa0\xa0\xa0additional_special_tokens:\xa0Optional[List[str]]\xa0=\xa0None,",(0,n.kt)("br",null),"\xa0\xa0\xa0\xa0**kwargs,",(0,n.kt)("br",null),")",(0,n.kt)("div",{className:"links-div"},(0,n.kt)("a",{href:"#HFBaseDataset-init",className:"direct-link"},"#"),(0,n.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/dataset/hf/base.py#L11-L42",className:"git-link"},"#"))),(0,n.kt)("div",{className:"api__description"}),(0,n.kt)("div",{className:"api"},(0,n.kt)("h4",{id:"HFBaseDataset-init_fields_batcher"},"init_fields_batcher"),(0,n.kt)("div",{className:"api__signature"},"def ",(0,n.kt)("span",{className:"ident"},"init_fields_batcher"),"(",(0,n.kt)("br",null),"\xa0\xa0\xa0\xa0self,",(0,n.kt)("br",null),") \u2011>\xa0Dict[~KT,\xa0~VT]",(0,n.kt)("div",{className:"links-div"},(0,n.kt)("a",{href:"#HFBaseDataset-init_fields_batcher",className:"direct-link"},"#"),(0,n.kt)("a",{href:"https://github.com/sunglasses-ai/classy/blob/6ce778ab1cf4a13f0f122a345f15beab4809551f/classy/data/dataset/hf/base.py#L41-L42",className:"git-link"},"#"))),(0,n.kt)("div",{className:"api__body"},(0,n.kt)("div",{className:"api__description"}))))))}p.isMDXComponent=!0}}]);