<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.16">
<link rel="search" type="application/opensearchdescription+xml" title="Classy" href="/classy/opensearch.xml"><title data-rh="true">Default Profiles | Classy</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://sunglasses-ai.github.io/classy/docs/reference-manual/profiles/"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Default Profiles | Classy"><meta data-rh="true" name="description" content="As you have seen from the previous tutorials, your systems are fully customizable in classy."><meta data-rh="true" property="og:description" content="As you have seen from the previous tutorials, your systems are fully customizable in classy."><link data-rh="true" rel="icon" href="/classy/img/classy_logo-short_transparent.png"><link data-rh="true" rel="canonical" href="https://sunglasses-ai.github.io/classy/docs/reference-manual/profiles/"><link data-rh="true" rel="alternate" href="https://sunglasses-ai.github.io/classy/docs/reference-manual/profiles/" hreflang="en"><link data-rh="true" rel="alternate" href="https://sunglasses-ai.github.io/classy/docs/reference-manual/profiles/" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://02OUF7W4JD-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/classy/assets/css/styles.62c90c90.css">
<link rel="preload" href="/classy/assets/js/runtime~main.606d26be.js" as="script">
<link rel="preload" href="/classy/assets/js/main.937572ff.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><div class="announcementBar_IbjG" role="banner"><div class="announcementBarPlaceholder_NC_W"></div><div class="announcementBarContent_KsVm">Classy is still in its early stages, help us shape its future by <a href="https://github.com/sunglasses-ai/classy#contributions">contributing</a>!</div><button type="button" class="clean-btn close announcementBarClose_FG1z" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/classy/"><div class="navbar__logo"><img src="/classy/img/CLASSY.svg" alt="Classy Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/classy/img/CLASSY.svg" alt="Classy Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Classy</b></a><a class="navbar__item navbar__link navbar__link--active" href="/classy/docs/intro/">Tutorial</a><a class="navbar__item navbar__link" href="/classy/docs/api/main/">Reference API</a><a href="https://github.com/sunglasses-ai/classy" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Classy@GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://github.com/sunglasses-ai/classy-template" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Template@GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://github.com/sunglasses-ai/classy-examples" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Examples@GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ğŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ğŸŒ</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode (currently light mode)"></div><div class="searchBox_qEbK"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO menuWithAnnouncementBar_x19h"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/classy/docs/intro/">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/classy/docs/installation/">Installation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/classy/docs/getting-started/basic/intro/">Getting started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/classy/docs/reference-manual/cli/train/">Reference Manual</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/classy/docs/reference-manual/cli/train/">classy CLI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/classy/docs/reference-manual/structured-configs/overall-structure/">Structured Configs</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/classy/docs/reference-manual/tasks-and-formats/">Tasks and Input Formats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/classy/docs/reference-manual/profiles/">Default Profiles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/classy/docs/reference-manual/mixins/">Mixins</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Xlws" aria-label="breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><span class="breadcrumbs__link breadcrumbsItemLink_e5ie">Reference Manual</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/classy/docs/reference-manual/profiles/">Default Profiles</a></li></ul></nav><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Default Profiles</h1></header><p>As you have seen from the previous tutorials, your systems are fully customizable in <code>classy</code>.
Even if we strongly encourage you to create you own configurations, we provide a set of predefined and well-established profiles
that will work with competitive performances in almost all setting and scenarios.</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p>To use a profile, you just have to pass the profile name to the parameter <code>--profile</code> at training time</p><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">task</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">dataset-path</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> -n </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">model-name</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> --profile </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">profile_name</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="distilbert--">distilbert ğŸŒ³ ğŸš€<a class="hash-link" href="#distilbert--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info">General Info<a class="hash-link" href="#general-info" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 4GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization">Model and Optimization<a class="hash-link" href="#model-and-optimization" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>DistilBERT</u> (ğŸ“„ <a href="https://arxiv.org/abs/1910.01108" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/distilbert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>Adafactor</u> (<a href="https://arxiv.org/abs/1804.04235" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adafactor-pytorch" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command">Train command<a class="hash-link" href="#train-command" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile distilbert</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>blazing fast</strong> training and inference</li><li><strong>Quick run</strong> to evaluate your dataset and check for possible flaws</li><li>You don&#x27;t have at your disposal a GPU with more than 4GB VRAM</li><li>You will use the model in <strong>low energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="distilroberta--">distilroberta ğŸŒ³ ğŸš€<a class="hash-link" href="#distilroberta--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-1">General Info<a class="hash-link" href="#general-info-1" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 4GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-1">Model and Optimization<a class="hash-link" href="#model-and-optimization-1" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>DistilRoBERTa</u> (ğŸ”¨ <a href="https://huggingface.co/distilroberta-base" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>Adafactor</u> (<a href="https://arxiv.org/abs/1804.04235" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adafactor-pytorch" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-1">Train command<a class="hash-link" href="#train-command-1" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile distilroberta</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-1">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-1" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>blazing fast</strong> training and inference</li><li><strong>Quick run</strong> to evaluate your dataset and check for possible flaws</li><li>You don&#x27;t have at your disposal a GPU with more than 4GB VRAM</li><li>You will use the model in <strong>low energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="squeezebert--">squeezebert ğŸŒ³ ğŸš€<a class="hash-link" href="#squeezebert--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-2">General Info<a class="hash-link" href="#general-info-2" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 4GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-2">Model and Optimization<a class="hash-link" href="#model-and-optimization-2" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>SqueezeBERT</u> (ğŸ“„ <a href="https://arxiv.org/abs/2006.11316" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/squeezebert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>Adafactor</u> (<a href="https://arxiv.org/abs/1804.04235" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adafactor-pytorch" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-2">Train command<a class="hash-link" href="#train-command-2" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile squeezebert</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-2">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-2" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>blazing fast</strong> training and inference</li><li><strong>Quick run</strong> to evaluate your dataset and check for possible flaws</li><li>You don&#x27;t have at your disposal a GPU with more than 4GB VRAM</li><li>You will use the model in <strong>low energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="bert-base--">bert-base ğŸŒ² ğŸš„<a class="hash-link" href="#bert-base--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-3">General Info<a class="hash-link" href="#general-info-3" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-3">Model and Optimization<a class="hash-link" href="#model-and-optimization-3" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>BERT-base</u> (ğŸ“„ <a href="https://aclanthology.org/N19-1423" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-3">Train command<a class="hash-link" href="#train-command-3" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bert-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-3">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-3" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want a <strong>well-established model</strong> for everyday use</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="gpt2--">gpt2 ğŸŒ² ğŸš„<a class="hash-link" href="#gpt2--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-4">General Info<a class="hash-link" href="#general-info-4" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>generation</code></td><td align="center">English</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-4">Model and Optimization<a class="hash-link" href="#model-and-optimization-4" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>GPT2</u> (ğŸ“„ <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/gpt2.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>Adam</u> (<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-4">Train command<a class="hash-link" href="#train-command-4" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">generation</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile gpt2</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-4">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-4" title="Direct link to heading">â€‹</a></h5><ul><li>You want an affordable (decoder-only) <strong>generative model</strong> for English</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="roberta-base--">roberta-base ğŸŒ² ğŸš„<a class="hash-link" href="#roberta-base--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-5">General Info<a class="hash-link" href="#general-info-5" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-5">Model and Optimization<a class="hash-link" href="#model-and-optimization-5" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>RoBERTa-base</u> (ğŸ“„ <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-5">Train command<a class="hash-link" href="#train-command-5" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bert-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-5">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-5" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want a <strong>well-established model</strong> for everyday use</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="deberta-base--">deberta-base ğŸŒ² ğŸš„<a class="hash-link" href="#deberta-base--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-6">General Info<a class="hash-link" href="#general-info-6" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-6">Model and Optimization<a class="hash-link" href="#model-and-optimization-6" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>DeBERTa-base</u> (ğŸ“„ <a href="https://arxiv.org/abs/2006.03654" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/deberta.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>RAdam</u> (<a href="https://arxiv.org/pdf/1908.03265v3.pdf" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://github.com/LiyuanLucasLiu/RAdam" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-6">Train command<a class="hash-link" href="#train-command-6" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile deberta-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-6">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-6" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want a <strong>recently released model with state-of-the-art performances</strong> on several NLU benchmarks</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="bart-base--">bart-base ğŸŒ² ğŸš„<a class="hash-link" href="#bart-base--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-7">General Info<a class="hash-link" href="#general-info-7" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code> <code>generation</code></td><td align="center">English</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-7">Model and Optimization<a class="hash-link" href="#model-and-optimization-7" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>Bart-base</u> (ğŸ“„ <a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bart.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>RAdam</u> (<a href="https://arxiv.org/pdf/1908.03265v3.pdf" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://github.com/LiyuanLucasLiu/RAdam" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-7">Train command<a class="hash-link" href="#train-command-7" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">generation</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bart-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-7">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-7" title="Direct link to heading">â€‹</a></h5><ul><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want to tackle an English generation task with an affordable model</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="multilingual-bert---">multilingual-bert ğŸŒ² ğŸš„ ğŸŒ<a class="hash-link" href="#multilingual-bert---" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-8">General Info<a class="hash-link" href="#general-info-8" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">104 (<a href="https://github.com/google-research/bert/blob/master/multilingual.md" target="_blank" rel="noopener noreferrer">Complete List</a>)</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-8">Model and Optimization<a class="hash-link" href="#model-and-optimization-8" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>mBERT</u> (ğŸ“„ <a href="https://aclanthology.org/N19-1423" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-8">Train command<a class="hash-link" href="#train-command-8" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile multilingual-bert</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-8">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-8" title="Direct link to heading">â€‹</a></h5><ul><li>You require a <strong>multilingual model</strong> covering languages other than English</li><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want a <strong>well-established model</strong> for everyday use</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="xlm-roberta-base---">xlm-roberta-base ğŸŒ² ğŸš„ ğŸŒ<a class="hash-link" href="#xlm-roberta-base---" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-9">General Info<a class="hash-link" href="#general-info-9" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">100 (Complete list in the reference paper)</td><td align="center">&lt; 8GB</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-9">Model and Optimization<a class="hash-link" href="#model-and-optimization-9" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>XLM-RoBERTa-base</u> (ğŸ“„ <a href="https://arxiv.org/abs/1911.02116" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/xlmroberta.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-9">Train command<a class="hash-link" href="#train-command-9" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile xlm-roberta-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-9">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-9" title="Direct link to heading">â€‹</a></h5><ul><li>You require a state-of-the-art <strong>multilingual model</strong> covering languages other than English</li><li>You want a <strong>trade-off between training/inference speed and model performances</strong></li><li>You want a <strong>well-established model</strong> for everyday use</li><li>You have at your disposal a GPU with at least 8GB of VRAM</li><li>You will use the model in <strong>moderate energy consumption</strong> scenarios</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="bert-large--">bert-large ğŸŒµ ğŸšœ<a class="hash-link" href="#bert-large--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-10">General Info<a class="hash-link" href="#general-info-10" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 11GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-10">Model and Optimization<a class="hash-link" href="#model-and-optimization-10" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>BERT-large</u> (ğŸ“„ <a href="https://aclanthology.org/N19-1423" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-10">Train command<a class="hash-link" href="#train-command-10" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bert-large --fp16</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>Remember to use the <code>--fp16</code> at training time or otherwise the model may not fit in memory.</p></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-10">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-10" title="Direct link to heading">â€‹</a></h5><ul><li>You want state-of-the-art performances, <strong>no compromise!</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want a <strong>well-established model</strong> used by thousands of users</li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="roberta-large--">roberta-large ğŸŒµ ğŸšœ<a class="hash-link" href="#roberta-large--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-11">General Info<a class="hash-link" href="#general-info-11" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 11GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-11">Model and Optimization<a class="hash-link" href="#model-and-optimization-11" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>RoBERTa-large</u> (ğŸ“„ <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bert.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-11">Train command<a class="hash-link" href="#train-command-11" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile roberta-large --fp16</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>Remember to use the <code>--fp16</code> at training time or otherwise the model may not fit in memory.</p></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-11">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-11" title="Direct link to heading">â€‹</a></h5><ul><li>You want state-of-the-art performances, <strong>no compromise!</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want a <strong>well-established model</strong> used by thousands of users</li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="deberta-large--">deberta-large ğŸŒµ ğŸšœ<a class="hash-link" href="#deberta-large--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-12">General Info<a class="hash-link" href="#general-info-12" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">English</td><td align="center">&lt; 11GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-12">Model and Optimization<a class="hash-link" href="#model-and-optimization-12" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>DeBERTa-large</u> (ğŸ“„ <a href="https://arxiv.org/abs/2006.03654" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/deberta.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>RAdam</u> (<a href="https://arxiv.org/pdf/1908.03265v3.pdf" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://github.com/LiyuanLucasLiu/RAdam" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-12">Train command<a class="hash-link" href="#train-command-12" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile deberta-large --fp16</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>Remember to use the <code>--fp16</code> at training time or otherwise the model may not fit in memory.</p></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-12">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-12" title="Direct link to heading">â€‹</a></h5><ul><li>You want state-of-the-art performances, <strong>no compromise!</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want <strong>one of the latest released SotA models</strong></li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="xlm-roberta-large---">xlm-roberta-large ğŸŒµ ğŸšœ ğŸŒ<a class="hash-link" href="#xlm-roberta-large---" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-13">General Info<a class="hash-link" href="#general-info-13" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code></td><td align="center">100 (Complete list in the reference paper)</td><td align="center">&lt; 16GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-13">Model and Optimization<a class="hash-link" href="#model-and-optimization-13" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>XLM-RoBERTa-large</u> (ğŸ“„ <a href="https://arxiv.org/abs/1911.02116" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/xlmroberta.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-13">Train command<a class="hash-link" href="#train-command-13" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile xlm-roberta-large --fp16</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>Remember to use the <code>--fp16</code> at training time or otherwise the model may not fit in memory.</p></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-13">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-13" title="Direct link to heading">â€‹</a></h5><ul><li>You require a state-of-the-art <strong>multilingual model</strong> covering languages other than English, with <strong>no compromise</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want a <strong>well-established model</strong> used by thousands of users</li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="gpt2-medium--">gpt2-medium ğŸŒµ ğŸšœ<a class="hash-link" href="#gpt2-medium--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-14">General Info<a class="hash-link" href="#general-info-14" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>generation</code></td><td align="center">English</td><td align="center">&lt; 11GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-14">Model and Optimization<a class="hash-link" href="#model-and-optimization-14" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>GPT2</u> (ğŸ“„ <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/gpt2.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-14">Train command<a class="hash-link" href="#train-command-14" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">generation</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile gpt2-medium</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-14">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-14" title="Direct link to heading">â€‹</a></h5><ul><li>You want a medium (decoder-only) <strong>generative model</strong> for English</li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="bart-large--">bart-large ğŸŒµ ğŸšœ<a class="hash-link" href="#bart-large--" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-15">General Info<a class="hash-link" href="#general-info-15" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code> <code>generation</code></td><td align="center">English</td><td align="center">&lt; 11GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-15">Model and Optimization<a class="hash-link" href="#model-and-optimization-15" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>Bart-large</u> (ğŸ“„ <a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/bart.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>RAdam</u> (<a href="https://arxiv.org/pdf/1908.03265v3.pdf" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://github.com/LiyuanLucasLiu/RAdam" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-15">Train command<a class="hash-link" href="#train-command-15" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">generation</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bart-large</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-15">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-15" title="Direct link to heading">â€‹</a></h5><ul><li>You want state-of-the-art performances, especially on English generation problems, with <strong>no compromise!</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want a <strong>well-established model</strong> used by thousands of users</li><li>You have at your disposal a GPU with at least 11GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="mbart--ï¸-">mbart ğŸŒµ ğŸ—ï¸ ğŸŒ<a class="hash-link" href="#mbart--ï¸-" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-16">General Info<a class="hash-link" href="#general-info-16" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>sequence</code> <code>sentence-pair</code> <code>token</code> <code>qa</code> <code>generation</code></td><td align="center">English</td><td align="center">&lt; 24GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-16">Model and Optimization<a class="hash-link" href="#model-and-optimization-16" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>mBART</u> (ğŸ“„ <a href="https://arxiv.org/abs/2001.08210" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/mbart.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>RAdam</u> (<a href="https://arxiv.org/pdf/1908.03265v3.pdf" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://github.com/LiyuanLucasLiu/RAdam" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-16">Train command<a class="hash-link" href="#train-command-16" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sequence</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">sentence-pair</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">qa</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile bart-base</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-16">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-16" title="Direct link to heading">â€‹</a></h5><ul><li>You want a state-of-the-art <strong>multilingual model</strong>, covering 25 languages and particularly suited for <strong>generation tasks</strong> (e.g. machine translation), with <strong>no compromise</strong></li><li>You want to show how far you can go with the proper infrastructure</li><li>You want a <strong>well-established model</strong> used by thousands of users</li><li>You have at your disposal a GPU with at least 24GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_mojV" id="gpt2-large--ï¸">gpt2-large ğŸŒµ ğŸ—ï¸<a class="hash-link" href="#gpt2-large--ï¸" title="Direct link to heading">â€‹</a></h2><h5 class="anchor anchorWithStickyNavbar_mojV" id="general-info-17">General Info<a class="hash-link" href="#general-info-17" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Supported Tasks</th><th align="center">Supported Languages</th><th align="center">Required VRAM</th></tr></thead><tbody><tr><td align="center"><code>generation</code></td><td align="center">English</td><td align="center">&lt; 24GB (fp16)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="model-and-optimization-17">Model and Optimization<a class="hash-link" href="#model-and-optimization-17" title="Direct link to heading">â€‹</a></h5><table><thead><tr><th align="center">Model</th><th align="center">Optimizer</th></tr></thead><tbody><tr><td align="center"><u>GPT2</u> (ğŸ“„ <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener noreferrer">Paper</a> <!-- -->|<!-- --> ğŸ”¨ <a href="https://huggingface.co/transformers/model_doc/gpt2.html" target="_blank" rel="noopener noreferrer">Implementation</a>)</td><td align="center"><u>AdamW</u> (<a href="https://arxiv.org/abs/1711.05101" target="_blank" rel="noopener noreferrer">Paper</a> ğŸ“„ <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" target="_blank" rel="noopener noreferrer">Implementation</a> ğŸ”¨)</td></tr></tbody></table><h5 class="anchor anchorWithStickyNavbar_mojV" id="train-command-17">Train command<a class="hash-link" href="#train-command-17" title="Direct link to heading">â€‹</a></h5><div class="codeBlockContainer_I0IT language-bash theme-code-block"><div class="codeBlockContent_wNvx bash"><pre tabindex="0" class="prism-code language-bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">classy train </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">generation</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> my_dataset_path -n my_model --profile gpt2-large</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h5 class="anchor anchorWithStickyNavbar_mojV" id="when-should-i-use-this-profile-17">When should I use this profile?<a class="hash-link" href="#when-should-i-use-this-profile-17" title="Direct link to heading">â€‹</a></h5><ul><li>You want a large (decoder-only) <strong>generative model</strong> for English</li><li>You have at your disposal a GPU with at least 24GB of VRAM that supports fp16 precision</li><li>You don&#x27;t have any energy consumption restriction</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/sunglasses-ai/classy/edit/main/docs/reference-manual/profiles.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/classy/docs/reference-manual/tasks-and-formats/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Tasks and Input Formats</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/classy/docs/reference-manual/mixins/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Mixins</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#distilbert--" class="table-of-contents__link toc-highlight">distilbert ğŸŒ³ ğŸš€</a></li><li><a href="#distilroberta--" class="table-of-contents__link toc-highlight">distilroberta ğŸŒ³ ğŸš€</a></li><li><a href="#squeezebert--" class="table-of-contents__link toc-highlight">squeezebert ğŸŒ³ ğŸš€</a></li><li><a href="#bert-base--" class="table-of-contents__link toc-highlight">bert-base ğŸŒ² ğŸš„</a></li><li><a href="#gpt2--" class="table-of-contents__link toc-highlight">gpt2 ğŸŒ² ğŸš„</a></li><li><a href="#roberta-base--" class="table-of-contents__link toc-highlight">roberta-base ğŸŒ² ğŸš„</a></li><li><a href="#deberta-base--" class="table-of-contents__link toc-highlight">deberta-base ğŸŒ² ğŸš„</a></li><li><a href="#bart-base--" class="table-of-contents__link toc-highlight">bart-base ğŸŒ² ğŸš„</a></li><li><a href="#multilingual-bert---" class="table-of-contents__link toc-highlight">multilingual-bert ğŸŒ² ğŸš„ ğŸŒ</a></li><li><a href="#xlm-roberta-base---" class="table-of-contents__link toc-highlight">xlm-roberta-base ğŸŒ² ğŸš„ ğŸŒ</a></li><li><a href="#bert-large--" class="table-of-contents__link toc-highlight">bert-large ğŸŒµ ğŸšœ</a></li><li><a href="#roberta-large--" class="table-of-contents__link toc-highlight">roberta-large ğŸŒµ ğŸšœ</a></li><li><a href="#deberta-large--" class="table-of-contents__link toc-highlight">deberta-large ğŸŒµ ğŸšœ</a></li><li><a href="#xlm-roberta-large---" class="table-of-contents__link toc-highlight">xlm-roberta-large ğŸŒµ ğŸšœ ğŸŒ</a></li><li><a href="#gpt2-medium--" class="table-of-contents__link toc-highlight">gpt2-medium ğŸŒµ ğŸšœ</a></li><li><a href="#bart-large--" class="table-of-contents__link toc-highlight">bart-large ğŸŒµ ğŸšœ</a></li><li><a href="#mbart--ï¸-" class="table-of-contents__link toc-highlight">mbart ğŸŒµ ğŸ—ï¸ ğŸŒ</a></li><li><a href="#gpt2-large--ï¸" class="table-of-contents__link toc-highlight">gpt2-large ğŸŒµ ğŸ—ï¸</a></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Links</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/classy/docs/intro/">Tutorial</a></li><li class="footer__item"><a class="footer__link-item" href="/classy/docs/installation/">Getting Started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/classy" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/sunglasses-ai/classy" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/sunglasses-ai/classy-template" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Template<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/sunglasses-ai/classy-examples" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Examples<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 sunglasses.ai.</div></div></div></footer></div>
<script src="/classy/assets/js/runtime~main.606d26be.js"></script>
<script src="/classy/assets/js/main.937572ff.js"></script>
</body>
</html>